{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jaehyeong/.conda/envs/pysgg/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pysgg.data.datasets import VGDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = None\n",
    "dict_file = None\n",
    "roidb_file = None\n",
    "image_file = None\n",
    "\n",
    "# Train Data\n",
    "train_data = VGDataset('train', img_dir, dict_file, roidb_file, image_file, filter_non_overlap=False)\n",
    "val_data = VGDataset('val', img_dir, dict_file, roidb_file, image_file)\n",
    "test_data = VGDataset('test', img_dir, dict_file, roidb_file, image_file)\n",
    "\n",
    "# object category dictionary\n",
    "cat_dicts = json.load(open('/home/public/Datasets/CV/vg_bm/VG-SGG-Category_v2.json', 'r'))\n",
    "\n",
    "# predicate dictionary\n",
    "predicate_dict = json.load(open('/home/public/Datasets/CV/vg_bm/VG-SGG-dicts-with-attri.json', 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'above',\n",
       " '2': 'across',\n",
       " '3': 'against',\n",
       " '4': 'along',\n",
       " '5': 'and',\n",
       " '6': 'at',\n",
       " '7': 'attached to',\n",
       " '8': 'behind',\n",
       " '9': 'belonging to',\n",
       " '10': 'between',\n",
       " '11': 'carrying',\n",
       " '12': 'covered in',\n",
       " '13': 'covering',\n",
       " '14': 'eating',\n",
       " '15': 'flying in',\n",
       " '16': 'for',\n",
       " '17': 'from',\n",
       " '18': 'growing on',\n",
       " '19': 'hanging from',\n",
       " '20': 'has',\n",
       " '21': 'holding',\n",
       " '22': 'in',\n",
       " '23': 'in front of',\n",
       " '24': 'laying on',\n",
       " '25': 'looking at',\n",
       " '26': 'lying on',\n",
       " '27': 'made of',\n",
       " '28': 'mounted on',\n",
       " '29': 'near',\n",
       " '30': 'of',\n",
       " '31': 'on',\n",
       " '32': 'on back of',\n",
       " '33': 'over',\n",
       " '34': 'painted on',\n",
       " '35': 'parked on',\n",
       " '36': 'part of',\n",
       " '37': 'playing',\n",
       " '38': 'riding',\n",
       " '39': 'says',\n",
       " '40': 'sitting on',\n",
       " '41': 'standing on',\n",
       " '42': 'to',\n",
       " '43': 'under',\n",
       " '44': 'using',\n",
       " '45': 'walking in',\n",
       " '46': 'walking on',\n",
       " '47': 'watching',\n",
       " '48': 'wearing',\n",
       " '49': 'wears',\n",
       " '50': 'with'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicate_dict['idx_to_predicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysgg.structures.image_list import to_image_list\n",
    "\n",
    "images = [torch.rand(3, 100, 200), torch.rand(3, 150, 170)]\n",
    "batched_images = to_image_list(images)\n",
    "\n",
    "# it is also possible to make the final batched image be a multiple of a number\n",
    "batched_images_32 = to_image_list(images, size_divisible=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 150, 200])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_images.tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pysgg.structures.bounding_box import BoxList\n",
    "FLIP_LEFT_RIGHT = 0\n",
    "\n",
    "width = 100\n",
    "height = 200\n",
    "boxes = [\n",
    "  [0, 10, 50, 50],\n",
    "  [50, 20, 90, 60],\n",
    "  [10, 10, 50, 50]\n",
    "]\n",
    "# create a BoxList with 3 boxes\n",
    "bbox = BoxList(boxes, image_size=(width, height), mode='xyxy')\n",
    "\n",
    "# perform some box transformations, has similar API as PIL.Image\n",
    "bbox_scaled = bbox.resize((width * 2, height * 3))\n",
    "bbox_flipped = bbox.transpose(FLIP_LEFT_RIGHT)\n",
    "\n",
    "# add labels for each bbox\n",
    "labels = torch.tensor([0, 10, 1])\n",
    "bbox.add_field('labels', labels)\n",
    "\n",
    "# bbox also support a few operations, like indexing\n",
    "# here, selects boxes 0 and 2\n",
    "bbox_subset = bbox[[0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoxList(num_boxes=3, image_width=100, image_height=200, mode=xyxy)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
